{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature-Extraction\n",
    "## Theoretische Grundlagen, Methodik und Umsetzung\n",
    "### KI in der Medizin: Brustkrebs Diagnose\n",
    "\n",
    "Wir haben den öffentlich verfügbaren Datensatz Breast Cancer Wisconsin verwendet und vom UCI Machine Learning Repository heruntergeladen. \n",
    "\n",
    "Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unser Ziel ist es, herauszufinden, welche Merkmale bei der Vorhersage von bösartigem oder gutartigem Krebs am hilfreichsten sind, und zu klassifizieren, ob der Brustkrebs gutartig oder bösartig ist.\n",
    "\n",
    "Wir haben den öffentlich verfügbaren Datensatz Breast Cancer Wisconsin verwendet und vom UCI Machine Learning Repository heruntergeladen.\n",
    "\n",
    "Die typische Leistungsanalyse wird durchgeführt\n",
    "\n",
    "![](conf_matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Bibliotheken importieren\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from cf_matrix import make_confusion_matrix\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, StratifiedKFold, GridSearchCV, cross_validate\n",
    "import matplotlib.pylab as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Download des Krebs-Datensatzes \n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "(X, y) = load_breast_cancer(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Entscheidungsbäumen Klassifikator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline in der Performance mit Entscheidungsbäumen\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from cf_matrix import make_confusion_matrix\n",
    "# Daten Skalierung\n",
    "t = MinMaxScaler()\n",
    "t.fit(X)\n",
    "X = t.transform(X)\n",
    "# Einfacher binärer Klassifikator\n",
    "model = XGBClassifier()\n",
    "#  definieren Sie das Verfahren der Kreuzvalidierung\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "# Modell auswerten\n",
    "scores = cross_validate(estimator=model, X=X, y=y, cv=cv, n_jobs=-1, \n",
    "                        scoring=['accuracy', 'roc_auc', 'precision', 'recall', 'f1'])\n",
    "print('Accuracy: ', scores['test_accuracy'].mean())\n",
    "print('Precision: ', scores['test_precision'].mean())\n",
    "print('Recall: ', scores['test_recall'].mean())\n",
    "print('F1: ', scores['test_f1'].mean(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manuelle Feature-Extraction\n",
    "\n",
    "## Korrelation\n",
    "\n",
    "Oft haben wir Features, die hoch korreliert sind und somit redundante Informationen liefern. Durch die Eliminierung hoch korrelierter Features können wir eine Vorhersageverzerrung für die in diesen Features enthaltenen Informationen vermeiden. \n",
    "Wir entfernen alle Merkmale mit einer Korrelation von mehr als 0,7.\n",
    "\n",
    "![](pearson_korr.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Download des Krebs-Datensatzes \n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "(X, y) = load_breast_cancer(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Korrelationanalyse\n",
    "X = X.iloc[:,1:-1]\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "X.iloc[:,0] = label_encoder.fit_transform(X.iloc[:,0]).astype('float64')\n",
    "corr = X.corr()\n",
    "sns.heatmap(corr)\n",
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.7:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "selected_columns = X.columns[columns]\n",
    "X = X[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überblick über die Daten\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline in der Performance mit XGBoost Modell\n",
    "# Einfacher binärer Klassifikator\n",
    "model = XGBClassifier()\n",
    "#  definieren Sie das Verfahren der Kreuzvalidierung\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "# Modell auswerten\n",
    "scores = cross_validate(estimator=model, X=X, y=y, cv=cv, n_jobs=-1, \n",
    "                        scoring=['accuracy', 'roc_auc', 'precision', 'recall', 'f1'])\n",
    "print('Accuracy: ', scores['test_accuracy'].mean())\n",
    "print('Precision: ', scores['test_precision'].mean())\n",
    "print('Recall: ', scores['test_recall'].mean())\n",
    "print('F1: ', scores['test_f1'].mean(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Datensatz in Trainings- und Testdatensatz aufteilen\n",
    "(X, y) = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, t_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Erstellen und Trainieren eines Autoencoders für die Klassifizierung mit Kompression in der Bottleneck-Schicht.\n",
    "# Anzahl der Eingangsspalten\n",
    "n_inputs = X.shape[1]\n",
    "# Skalierung der Daten\n",
    "t = MinMaxScaler()\n",
    "t.fit(X_train)\n",
    "X_train = t.transform(X_train)\n",
    "X_test = t.transform(X_test)\n",
    "# Encoder definieren\n",
    "visible = Input(shape=(n_inputs,))\n",
    "# Encoder Ebene 1\n",
    "e = Dense(n_inputs*2)(visible)\n",
    "e = BatchNormalization()(e)\n",
    "e = LeakyReLU()(e)\n",
    "# Encoder Ebene 2\n",
    "e = Dense(n_inputs)(e)\n",
    "e = BatchNormalization()(e)\n",
    "e = LeakyReLU()(e)\n",
    "# Bottleneck-Schicht\n",
    "n_bottleneck = round(float(n_inputs) / 2.0)\n",
    "bottleneck = Dense(n_bottleneck)(e)\n",
    "# Decoder definieren, Ebene 1\n",
    "d = Dense(n_inputs)(bottleneck)\n",
    "d = BatchNormalization()(d)\n",
    "d = LeakyReLU()(d)\n",
    "# Decoder Ebene 1\n",
    "d = Dense(n_inputs*2)(d)\n",
    "d = BatchNormalization()(d)\n",
    "d = LeakyReLU()(d)\n",
    "# Ausgabe\n",
    "output = Dense(n_inputs, activation='linear')(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufbau des Autoencoders\n",
    "![](autoencoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Autoencoder definieren\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# Kompilieren des Autoencoders\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# Fit das Autoencoder-Modell, um die Eingabe zu rekonstruieren\n",
    "history = model.fit(X_train, X_train, epochs=200, batch_size=10, verbose=2, validation_data=(X_test,X_test))\n",
    "# die Entwicklung der Verlustfunktion darstellen\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Testing')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# das Encodermodell vom Autoencoder trennen (ohne den Decoder)\n",
    "encoder = Model(inputs=visible, outputs=bottleneck)\n",
    "# Speichern des Encoders\n",
    "encoder.save('encoder.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufbau des Encoders\n",
    "![](encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder + Entscheidungsbäumen Klassifikator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entscheidungsbäumen auf kodierter Eingabe auswerten (Encoder + Entscheidungsbäumen Klassifikator)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "# Laden des Modells aus Datei\n",
    "encoder = load_model('encoder.h5')\n",
    "model = XGBClassifier()\n",
    "# die Trainingsdaten kodieren\n",
    "X_train_encode = encoder.predict(X)\n",
    "#  definieren Sie das Verfahren der Kreuzvalidierung\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "# Modell auswerten\n",
    "scores = cross_validate(estimator=model, X=X_train_encode, y=y, cv=cv, n_jobs=-1, \n",
    "                        scoring=['accuracy', 'roc_auc', 'precision', 'recall', 'f1'])\n",
    "print('Accuracy: ', scores['test_accuracy'].mean())\n",
    "print('Precision: ', scores['test_precision'].mean())\n",
    "print('Recall: ', scores['test_recall'].mean())\n",
    "print('F1: ', scores['test_f1'].mean(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA + Entscheidungsbäumen Klassifikator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klassifizierung auf reduzierter Dimension (PCA-Merkmalsextrahierung + Entscheidungsbäumenklassifikator)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Liste der Transformationen an den Daten\n",
    "transforms = list()\n",
    "transforms.append(('pca', PCA(n_components=9)))\n",
    "fu = FeatureUnion(transforms)\n",
    "# das Modell definieren\n",
    "model = XGBClassifier()\n",
    "# die Pipeline definieren\n",
    "steps = list()\n",
    "steps.append(('fu', fu))\n",
    "steps.append(('m', model))\n",
    "pipeline = Pipeline(steps=steps)\n",
    "#  definieren Sie das Verfahren der Kreuzvalidierung\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "# Modell auswerten\n",
    "scores = cross_validate(estimator=pipeline, X=X, y=y, cv=cv, n_jobs=-1, \n",
    "                        scoring=['accuracy', 'roc_auc', 'precision', 'recall', 'f1'])\n",
    "print('Accuracy: ', scores['test_accuracy'].mean())\n",
    "print('Precision: ', scores['test_precision'].mean())\n",
    "print('Recall: ', scores['test_recall'].mean())\n",
    "print('F1: ', scores['test_f1'].mean(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Über diesen Github-Link können Sie auf den gesamten Code und die Daten des Projekts zugreifen.\n",
    "\n",
    "![](download.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
